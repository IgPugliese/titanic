{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "project_root = Path().resolve().parent\n",
    "sys.path.append(str(project_root))\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import xgboost as xgb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import sklearn as skt\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, StratifiedKFold\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from xgboost import XGBRegressor, XGBClassifier\n",
    "from sklearn.preprocessing import FunctionTransformer\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.base import clone\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from noise_remover import NoiseRemover\n",
    "from titanic_preprocessor import TitanicPreprocessor\n",
    "from sklearn.model_selection import PredefinedSplit, GridSearchCV\n",
    "from xgboost.callback import EarlyStopping\n",
    "\n",
    "from utils import load_config, load_datasets\n",
    "\n",
    "load_config()\n",
    "dataset, dt = load_datasets()\n",
    "\n",
    "model= XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",       # modela conteos\n",
    "    tree_method=\"hist\",         # rápido y eficiente en CPU\n",
    "    n_estimators=5000,\n",
    "    learning_rate=0.1,\n",
    "    callbacks= [EarlyStopping(rounds=50, save_best=True)],\n",
    "    max_depth=3,\n",
    "    enable_categorical=True,\n",
    "    verbosity=0\n",
    ")\n",
    "\n",
    "def generate_predefined_split(y, prep):\n",
    "    X_train, X_eval, y_train, y_eval = train_test_split(df, y, test_size=0.2, stratify=y)\n",
    "    X_train_preprocessed = prep.fit_transform(X_train, y_train)  \n",
    "    X_eval_preprocessed  = prep.transform(X_eval)   \n",
    "    X_preprocessed = pd.concat([X_train_preprocessed, X_eval_preprocessed], axis=0)\n",
    "    y_all   = pd.concat([y_train, y_eval], axis=0)\n",
    "    test_fold = np.r_[[-1]*len(X_train_preprocessed), [0]*len(X_eval_preprocessed)]\n",
    "    return X_preprocessed, X_eval_preprocessed, y_all, y_eval, test_fold\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2,14],\n",
    "    'learning_rate': [0.1, 0.2],\n",
    "    'subsample': [0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.9]\n",
    "}\n",
    "\n",
    "df=dataset.copy()\n",
    "y = df.pop('Survived')\n",
    "dfcv=df.copy()\n",
    "params=[]\n",
    "\n",
    "for i in range(100):\n",
    "    data_preprocessor = TitanicPreprocessor()\n",
    "\n",
    "    prep = Pipeline([('pre-processor', TitanicPreprocessor()),('noise_remover', NoiseRemover())])\n",
    "    X_preprocessed, X_eval_preprocessed, y_all, y_eval, test_fold = generate_predefined_split(y, prep)\n",
    "\n",
    "    grid_search = GridSearchCV(\n",
    "        estimator=model,\n",
    "        param_grid=param_grid,\n",
    "        cv=PredefinedSplit(test_fold),\n",
    "        scoring='accuracy',\n",
    "        n_jobs=-1,  # Use all available cores\n",
    "        verbose=0  # Print progress\n",
    "    )\n",
    "\n",
    "    print(\"Starting grid search...\")\n",
    "    grid_search.fit(X_preprocessed, y_all,eval_set=[(X_eval_preprocessed, y_eval)],verbose=False)\n",
    "\n",
    "    best_xgb = grid_search.best_estimator_\n",
    "    best_iter = getattr(best_xgb, \"best_iteration\", None)\n",
    "    print(\"Best params:\", grid_search.best_params_)\n",
    "    print(\"Best (hold-out) score:\", grid_search.best_score_)\n",
    "    n_estimators=(best_iter + 1) if best_iter is not None else 0\n",
    "    print(\"Best n_estimators (ES):\", n_estimators)\n",
    "    grid_search.best_params_[\"n_estimators\"]=n_estimators\n",
    "    grid_search.best_params_[\"score\"]=grid_search.best_score_\n",
    "    params.append(grid_search.best_params_)\n",
    "\n",
    "paramsFileContent=pd.DataFrame(params)\n",
    "paramsFileContent.to_csv(\"params.csv\", index=False)\n",
    "\n",
    "print(params)\n",
    "\n",
    "#[{'colsample_bytree': 0.8, 'learning_rate': 0.8, 'max_depth': 9, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'subsample': 0.5}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'rfbes__max_depth': [3, 4, 5, 7, 9 ,12, 13, 14, 15],\n",
    "    'rfbes__learning_rate': [0.008,0.009 ,0.01, 0.1, 0.2, 0.8],\n",
    "    'rfbes__subsample': [0.5, 0.8, 0.9, 1.0],\n",
    "    'rfbes__colsample_bytree': [0.3, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "df=dataset.copy()\n",
    "y = df.pop('Survived')\n",
    "dfcv=df.copy()\n",
    "params=[]\n",
    "\n",
    "\n",
    "class ModelWithEarlyStop(BaseEstimator, ClassifierMixin):\n",
    "\n",
    "    def __init__(self, n_estimators=5000,\n",
    "                 learning_rate=0.1,\n",
    "                 max_depth=3,\n",
    "                 subsample=1.0,\n",
    "                 colsample_bytree=1.0):\n",
    "        self.n_estimators = n_estimators\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_depth = max_depth\n",
    "        self.subsample = subsample\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.model=XGBClassifier(\n",
    "        objective=\"binary:logistic\",\n",
    "        eval_metric=\"logloss\",       # modela conteos\n",
    "        tree_method=\"hist\",         # rápido y eficiente en CPU\n",
    "        n_estimators=5000,\n",
    "        learning_rate=0.1,\n",
    "        callbacks= [EarlyStopping(rounds=50, save_best=True)],\n",
    "        max_depth=3,\n",
    "        enable_categorical=True,\n",
    "        verbosity=0\n",
    "    )\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X, y, eval_set=[(X, y)], verbose=0)\n",
    "        print(f\"llamando fit modelo con {len(X):d}\")\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        self.model.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model_.predict_proba(X)\n",
    "        \n",
    "\n",
    "cv=StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "data_preprocessor = TitanicPreprocessor()\n",
    "model_with_ES = ModelWithEarlyStop(cv)\n",
    "pipeline = Pipeline([('pre-processor', data_preprocessor), ('nosie remover', NoiseRemover()),('rfbes', model_with_ES)])\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=pipeline,\n",
    "    param_grid=param_grid,\n",
    "    cv=cv,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=0  # Print progress\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(df, y)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_iter = getattr(best_xgb, \"best_iteration\", None)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "print(\"Best (hold-out) score:\", grid_search.best_score_)\n",
    "n_estimators=(best_iter + 1) if best_iter is not None else 0\n",
    "print(\"Best n_estimators (ES):\", n_estimators)\n",
    "grid_search.best_params_[\"n_estimators\"]=n_estimators\n",
    "grid_search.best_params_[\"score\"]=grid_search.best_score_\n",
    "params.append(grid_search.best_params_)\n",
    "\n",
    "paramsFileContent=pd.DataFrame(params)\n",
    "paramsFileContent.to_csv(\"params.csv\", index=False)\n",
    "\n",
    "print(params)\n",
    "\n",
    "#[{'colsample_bytree': 0.8, 'learning_rate': 0.8, 'max_depth': 9, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'subsample': 0.5}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'max_depth': [2, 3, 4, 5],\n",
    "    'learning_rate': [0.009 ,0.01, 0.1, 0.2, 0.25, 0.28, 0.30, 0.4],\n",
    "    'subsample': [0.4, 0.5, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.8, 0.85, 0.9, 1.0],\n",
    "    'reg_alpha': [2, 5, 10, 15, 20],\n",
    "    'reg_lambda': [0.00]\n",
    "}\n",
    "\n",
    "df=dataset.copy()\n",
    "y = df.pop('Survived')\n",
    "dfcv=df.copy()\n",
    "params=[]\n",
    "\n",
    "class XGBoostCVWrapper(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,max_depth=7, learning_rate=0.01, subsample=0.9, colsample_bytree=0.9, n_estimators=5000, reg_lambda=2,  reg_alpha=0.0):\n",
    "        self.max_depth = max_depth\n",
    "        self.reg_lambda = reg_lambda\n",
    "        self.reg_alpha = reg_alpha\n",
    "        self.subsample = subsample\n",
    "        self.n_estimators = n_estimators\n",
    "        self.colsample_bytree = colsample_bytree\n",
    "        self.learning_rate = learning_rate\n",
    "        self.cv_results_ = None\n",
    "        self.best_iteration_ = None\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        dtrain = xgb.DMatrix(X, label=y, enable_categorical=True)\n",
    "        self.cv_results_ = xgb.cv(\n",
    "        params={'max_depth': self.max_depth, 'learning_rate': self.learning_rate, 'subsample': self.subsample, 'colsample_bytree': self.colsample_bytree,  'objective': 'binary:logistic', 'reg_lambda': self.reg_lambda,  'reg_alpha': self.reg_alpha},  # objective needed for regression\n",
    "        dtrain=dtrain,\n",
    "        num_boost_round=5000,\n",
    "        nfold=5,\n",
    "        metrics='logloss',\n",
    "        early_stopping_rounds=50,\n",
    "        seed=42,\n",
    "        as_pandas=True,\n",
    "        verbose_eval=False\n",
    "        )\n",
    "        self.best_iteration_ = len(self.cv_results_)\n",
    "\n",
    "        cv=StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "\n",
    "        self.model_ = xgb.XGBClassifier(\n",
    "            max_depth=self.max_depth,\n",
    "            learning_rate=self.learning_rate,\n",
    "            subsample=self.subsample,\n",
    "            colsample_bytree=self.colsample_bytree,\n",
    "            n_estimators=self.best_iteration_,\n",
    "            enable_categorical=True,\n",
    "            eval_metric='logloss',\n",
    "            tree_method=\"hist\",  \n",
    "            reg_lambda=self.reg_lambda,            # L2 ayuda a estabilizar\n",
    "            reg_alpha=self.reg_alpha,\n",
    "        )\n",
    "\n",
    "        \n",
    "        pipeline = Pipeline([('pre-processor', data_preprocessor), ('nosie remover', NoiseRemover()),('rfb', self.model_)])\n",
    "\n",
    "\n",
    "        self.score_val = cross_val_score(pipeline, dfcv,y,cv=cv,scoring=\"neg_log_loss\").mean()\n",
    "        self.model_.fit(X, y)\n",
    "        self.best_result_ = self.cv_results_['test-logloss-mean'].min()\n",
    "\n",
    "        print(f\"'max_depth': {self.max_depth}, 'learning_rate': {self.learning_rate}, 'subsample': {self.subsample}, 'colsample_bytree': {self.colsample_bytree} 'n_estimators': {self.best_iteration_} , result: {self.score_val}\")\n",
    "        return self\n",
    "    \n",
    "    \n",
    "    def score(self, X, y):\n",
    "        return self.score_val\n",
    "    \n",
    "    \n",
    "data_preprocessor = TitanicPreprocessor()\n",
    "\n",
    "modelo_falopa = XGBoostCVWrapper()\n",
    "prep = Pipeline([('pre-processor', TitanicPreprocessor()),('noise_remover', NoiseRemover())])\n",
    "\n",
    "prep.fit(df)\n",
    "X_preprocessed = prep.transform(df)\n",
    "dummy_cv = [(np.arange(len(X_preprocessed)), np.arange(len(X_preprocessed)))]\n",
    "\n",
    "\n",
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo_falopa,\n",
    "    param_grid=param_grid,\n",
    "    scoring=None,\n",
    "    cv=dummy_cv,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=0  # Print progress\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(X_preprocessed, y)\n",
    "print(\"Best score:\", grid_search.best_score_)\n",
    "print(\"Best parameters:\", grid_search.best_params_)\n",
    "print()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#[{'colsample_bytree': 0.8, 'learning_rate': 0.8, 'max_depth': 9, 'subsample': 0.5}, {'colsample_bytree': 0.8, 'learning_rate': 0.01, 'max_depth': 5, 'subsample': 0.5}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_search = GridSearchCV(\n",
    "    estimator=modelo_falopa,\n",
    "    param_grid=param_grid,\n",
    "    scoring=None,\n",
    "    cv=dummy_cv,\n",
    "    n_jobs=-1,  # Use all available cores\n",
    "    verbose=0  # Print progress\n",
    ")\n",
    "\n",
    "print(\"Starting grid search...\")\n",
    "grid_search.fit(X_preprocessed, y_all)\n",
    "\n",
    "best_xgb = grid_search.best_estimator_\n",
    "best_iter = getattr(best_xgb, \"best_iteration\", None)\n",
    "print(\"Best params:\", grid_search.best_params_)\n",
    "n_estimators=(best_iter + 1) if best_iter is not None else 0\n",
    "print(\"Best n_estimators (ES):\", grid_search.best_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "params_and_results=[]\n",
    "cv=StratifiedKFold(n_splits=5,shuffle=False,random_state=None)\n",
    "for setOfParams in params:\n",
    "    tunedModel= XGBClassifier(\n",
    "    objective=\"binary:logistic\",\n",
    "    eval_metric=\"logloss\",       # modela conteos\n",
    "    tree_method=\"hist\",         # rápido y eficiente en CPU\n",
    "    n_estimators=setOfParams[\"n_estimators\"],\n",
    "    learning_rate=setOfParams[\"learning_rate\"],\n",
    "    max_depth=setOfParams[\"max_depth\"],\n",
    "    colsample_bytree=setOfParams[\"colsample_bytree\"],\n",
    "    subsample=setOfParams[\"subsample\"],\n",
    "    enable_categorical=True,\n",
    "    verbosity=0\n",
    "    )\n",
    "    pipeline = Pipeline([('pre-processor', data_preprocessor), ('nosie remover', NoiseRemover()),('rfb', tunedModel)])\n",
    "    final_score_raw=cross_val_score(pipeline,dfcv,y,cv=cv,scoring=\"accuracy\")\n",
    "    setOfParams[\"cross_validation_score\"] = final_score_raw.mean()\n",
    "    setOfParams[\"cross_validation_error_margin\"] = final_score_raw.std()\n",
    "    params_and_results.append(setOfParams)\n",
    "    print(\"para los siguientes hiperparametros\")\n",
    "    print(setOfParams)\n",
    "    print(f\"El promedio de precision en cross validation es: {final_score_raw.mean():.4f} (+/- {final_score_raw.std() * 2:.4f})\")\n",
    "\n",
    "\n",
    "\n",
    "params_and_results.sort(key=lambda x : x[\"cross_validation_score\"])    \n",
    "paramsFileContent=pd.DataFrame(params_and_results)\n",
    "paramsFileContent.to_csv(\"params_with_results.csv\", index=False)\n",
    " \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deadlyAliance",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
